{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce2631-b840-4a7f-8183-fc50cb1977ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark aggregation functions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a3193-f1a5-4e85-b50a-f80cf5908b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = spark.read.csv(\"../data/listings.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\", \n",
    "    quote='\"',\n",
    "    escape='\"', \n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\" \n",
    ")\n",
    "listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32095599-a1da-408e-b315-3e0481e8bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.csv(\"../data/reviews.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fc7ba-8be1-4680-aaf1-6724d1399e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. For each listing compute string category depending on its price, and add it as a new column.\n",
    "# A category is defined in the following way:\n",
    "#\n",
    "# * price < 50 -> \"Budget\"\n",
    "# * 50 <= price < 150 -> \"Mid-range\"\n",
    "# * price >= 150 -> \"Luxury\"\n",
    "# \n",
    "# Only include listings where the price is not null.\n",
    "# Count the number of listings in each category\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "listings = listings.withColumn('price_numeric', regexp_replace('price', '[$,]', '').cast('float'))\n",
    "\n",
    "# TODO: Implement a UDF\n",
    "# TODO: Apply the UDF to create a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6d82e-6255-40bb-be8f-837c0cef6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. In this task you will need to compute a santiment score per review, and then an average sentiment score per listing.\n",
    "# A santiment score indicates how \"positive\" or \"negative\" a review is. The higher the score the more positive it is, and vice-versa.\n",
    "#\n",
    "# To compute a sentiment score per review compute the number of positive words in a review and subtract the number of negative\n",
    "# words in the same review (the list of words is already provided)\n",
    "#\n",
    "# To complete this task, compute a DataFrame that contains the following fields:\n",
    "# * name - the name of a listing\n",
    "# * average_sentiment - average sentiment of reviews computed using the algorithm described above\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Lists of positive and negative words\n",
    "positive_words = {'good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'pleasant', 'lovely', 'nice', 'enjoyed'}\n",
    "negative_words = {'bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'hate', 'unpleasant', 'dirty', 'noisy'}\n",
    "\n",
    "# TODO: Implement the UDF\n",
    "def sentiment_score(comment):\n",
    "    pass\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "reviews_with_sentiment = reviews \\\n",
    "  .withColumn(\n",
    "    'sentiment_score',\n",
    "    sentiment_score_udf(reviews.comments)\n",
    "  )\n",
    "\n",
    "# TODO: Create a final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b15b2-66df-4e9b-9bc1-8ba328e14aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rewrite the following code from the previous exercise using SparkSQL:\n",
    "#\n",
    "# ```\n",
    "# from pyspark.sql.functions import length, avg, count\n",
    "# \n",
    "# reviews_with_comment_length = reviews.withColumn('comment_length', length('comments'))\n",
    "# reviews_with_comment_length \\\n",
    "#   .join(listings, reviews_with_comment_length.listing_id == listings.id, 'inner') \\\n",
    "#   .groupBy('listing_id').agg(\n",
    "#       avg(reviews_with_comment_length.comment_length).alias('average_comment_length'),\n",
    "#       count(reviews_with_comment_length.id).alias('reviews_count')\n",
    "#   ) \\\n",
    "#   .filter('reviews_count >= 5') \\\n",
    "#   .orderBy('average_comment_length', ascending=False) \\\n",
    "#   .show()\n",
    "# ```\n",
    "# This was a solution for the the task:\n",
    "#\n",
    "# \"Get top five listings with the highest average review comment length. Only return listings with at least 5 reviews\"\n",
    "\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "listings.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "spark \\\n",
    "  .sql(sql_query) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd68c71-0ce8-4a21-be62-a822fce18522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. [Optional][Challenge]\n",
    "# Calculate an average time passed from the first review for each host in the listings dataset. \n",
    "# To implmenet a custom aggregation function you would need to use \"pandas_udf\" function to write a custom aggregation function.\n",
    "#\n",
    "# Documentation about \"pandas_udf\": https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html \n",
    "#\n",
    "# To use \"pandas_udf\" you would need to install two additional dependencies in the virtual environment you use for PySpark:\n",
    "# Run these commands:\n",
    "# ```\n",
    "# pip install pandas\n",
    "# pip install pyarrow\n",
    "# ```\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def average_days_since_first_review_udf(first_review_series) -> float:\n",
    "    # TODO: Implement the UDF\n",
    "    pass\n",
    "\n",
    "listings \\\n",
    "  .filter(\n",
    "    listings.first_review.isNotNull()\n",
    "  ) \\\n",
    "  .groupBy('host_id') \\\n",
    "  .agg(\n",
    "    average_days_since_first_review_udf(listings.first_review).alias('average_days_since_first_review_days')\n",
    "  ) \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
