{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ce2631-b840-4a7f-8183-fc50cb1977ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 23:00:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark aggregation functions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078a3193-f1a5-4e85-b50a-f80cf5908b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- last_scraped: date (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: date (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: date (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- first_review: date (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings = spark.read.csv(\"../data/listings.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\", \n",
    "    quote='\"',\n",
    "    escape='\"', \n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\" \n",
    ")\n",
    "listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32095599-a1da-408e-b315-3e0481e8bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.csv(\"../data/reviews.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5fc7ba-8be1-4680-aaf1-6724d1399e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|price_category|count|\n",
      "+--------------+-----+\n",
      "|     Mid-range|29562|\n",
      "|        Budget| 5995|\n",
      "|        Luxury|27648|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. For each listing compute string category depending on its price, and add it as a new column.\n",
    "# A category is defined in the following way:\n",
    "#\n",
    "# * price < 50 -> \"Budget\"\n",
    "# * 50 <= price < 150 -> \"Mid-range\"\n",
    "# * price >= 150 -> \"Luxury\"\n",
    "# \n",
    "# Only include listings where the price is not null.\n",
    "# Count the number of listings in each category\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "listings = listings.withColumn('price_numeric', regexp_replace('price', '[$,]', '').cast('float'))\n",
    "\n",
    "def categorize_price(price):\n",
    "    if price is None:\n",
    "        return 'Unknown'\n",
    "    elif price < 50:\n",
    "        return 'Budget'\n",
    "    elif 50 <= price < 150:\n",
    "        return 'Mid-range'\n",
    "    elif price >= 150:\n",
    "        return 'Luxury'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "categorize_price_udf = udf(categorize_price, StringType())\n",
    "\n",
    "listings_with_category = listings \\\n",
    "  .filter(listings.price_numeric.isNotNull()) \\\n",
    "  .withColumn(\n",
    "    'price_category',\n",
    "    categorize_price_udf(listings.price_numeric)\n",
    "  ) \\\n",
    "  .groupBy('price_category') \\\n",
    "  .count() \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b6d82e-6255-40bb-be8f-837c0cef6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------+-----------------+\n",
      "|listing_id         |name                                              |average_sentiment|\n",
      "+-------------------+--------------------------------------------------+-----------------+\n",
      "|8630729            |Central and Cozy 2 BR Flat next to Pimlico station|6.0              |\n",
      "|1210552487630259444|Cozy flat in London                               |6.0              |\n",
      "|1213675403351699745|Knightsbridge Two Bedroom Duplex                  |5.0              |\n",
      "|3804150            |London NW3. Feel at home single room              |5.0              |\n",
      "|15488286           |Canal Side stylish 1 Bedroom Apartment near tube  |5.0              |\n",
      "|945592509209998667 |Cozy One Bedroom Full Flat                        |5.0              |\n",
      "|24763465           |Luxury Holiday Let | Prime SW19 Village Location  |5.0              |\n",
      "|1091966865591580198|Stunning 3 bedroom apartment in Surbiton          |5.0              |\n",
      "|1169478573487073390|Spacious Victorian Terrace with Tropical Garden   |5.0              |\n",
      "|39575614           |Light-filled 1 bed Victorian flat in Canonbury    |5.0              |\n",
      "|34277415           |Gorgeous family home in Fulham for 6 by the river |5.0              |\n",
      "|14577353           |En suite double room in House of Zen!             |5.0              |\n",
      "|40386257           |Beautifully Contemporary Three Bedroom House      |5.0              |\n",
      "|1188420993354944657|Chic Stay with Garden, 18 mins into City          |5.0              |\n",
      "|649681180673316782 |Very Modern Double Room With Private Bathroom     |5.0              |\n",
      "|569323930307899788 |Blueground | Fitzrovia, nr regent & oxford st     |5.0              |\n",
      "|1188569910646630124|1 bed flat with private garden                    |5.0              |\n",
      "|1171806468687979023|House, 7min from station & park.                  |5.0              |\n",
      "|41336114           |Charming Period one Bedroom Garden Apartment      |5.0              |\n",
      "|51832910           |(4) Stunning Lux Modern Flat - Great Connections  |5.0              |\n",
      "+-------------------+--------------------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. In this task you will need to compute a santiment score per review, and then an average sentiment score per listing.\n",
    "# A santiment score indicates how \"positive\" or \"negative\" a review is. The higher the score the more positive it is, and vice-versa.\n",
    "#\n",
    "# To compute a sentiment score per review compute the number of positive words in a review and subtract the number of negative\n",
    "# words in the same review (the list of words is already provided)\n",
    "#\n",
    "# To complete this task, compute a DataFrame that contains the following fields:\n",
    "# * name - the name of a listing\n",
    "# * average_sentiment - average sentiment of reviews computed using the algorithm described above\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Lists of positive and negative words\n",
    "positive_words = {'good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'pleasant', 'lovely', 'nice', 'enjoyed'}\n",
    "negative_words = {'bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'hate', 'unpleasant', 'dirty', 'noisy'}\n",
    "\n",
    "def sentiment_score(comment):\n",
    "    if comment is None:\n",
    "        return 0.0\n",
    "    comment_lower = comment.lower()\n",
    "    score = 0\n",
    "    \n",
    "    for word in positive_words:\n",
    "        if word in comment_lower:\n",
    "            score += 1\n",
    "            \n",
    "    for word in negative_words:\n",
    "        if word in comment_lower:\n",
    "            score -= 1\n",
    "    return float(score)\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "reviews_with_sentiment = reviews \\\n",
    "  .withColumn(\n",
    "    'sentiment_score',\n",
    "    sentiment_score_udf(reviews.comments)\n",
    "  )\n",
    "\n",
    "listings \\\n",
    "   .join(reviews_with_sentiment, listings.id == reviews.listing_id, 'inner') \\\n",
    "   .groupBy('listing_id', 'name') \\\n",
    "   .agg(\n",
    "      avg('sentiment_score').alias('average_sentiment')\n",
    "   ) \\\n",
    "   .orderBy('average_sentiment', ascending=False) \\\n",
    "   .select('listing_id', 'name', 'average_sentiment') \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637b15b2-66df-4e9b-9bc1-8ba328e14aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 23:01:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+-------------+\n",
      "|        listing_id|average_comment_length|reviews_count|\n",
      "+------------------+----------------------+-------------+\n",
      "|618608352812465378|    1300.1666666666667|            6|\n",
      "|627425975703032358|    1190.7142857142858|            7|\n",
      "|          28508447|    1089.3333333333333|            6|\n",
      "|          42776409|                1089.2|            5|\n",
      "|          40439200|     1074.857142857143|            7|\n",
      "|           2197681|                 939.2|            5|\n",
      "|          13891813|                 905.0|            5|\n",
      "|            979753|     893.9230769230769|           13|\n",
      "|630150178279666225|     890.7272727272727|           11|\n",
      "|           8856894|     890.1666666666666|            6|\n",
      "|          29469389|                 885.0|            6|\n",
      "|          22524075|                 885.0|            5|\n",
      "|           5555679|     878.7169811320755|          106|\n",
      "|          41515075|                 875.5|            6|\n",
      "|          54364596|     870.6666666666666|            6|\n",
      "|          33385444|                 848.0|            5|\n",
      "|            565214|     834.0833333333334|           12|\n",
      "|          53493254|                 831.0|            7|\n",
      "|          12646480|                 819.6|            5|\n",
      "|          17997858|               816.875|            8|\n",
      "+------------------+----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. Rewrite the following code from the previous exercise using SparkSQL:\n",
    "#\n",
    "# ```\n",
    "# from pyspark.sql.functions import length, avg, count\n",
    "# \n",
    "# reviews_with_comment_length = reviews.withColumn('comment_length', length('comments'))\n",
    "# reviews_with_comment_length \\\n",
    "#   .join(listings, reviews_with_comment_length.listing_id == listings.id, 'inner') \\\n",
    "#   .groupBy('listing_id').agg(\n",
    "#       avg(reviews_with_comment_length.comment_length).alias('average_comment_length'),\n",
    "#       count(reviews_with_comment_length.id).alias('reviews_count')\n",
    "#   ) \\\n",
    "#   .filter('reviews_count >= 5') \\\n",
    "#   .orderBy('average_comment_length', ascending=False) \\\n",
    "#   .show()\n",
    "# ```\n",
    "# This was a solution for the the task:\n",
    "#\n",
    "# \"Get top five listings with the highest average review comment length. Only return listings with at least 5 reviews\"\n",
    "\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "listings.createOrReplaceTempView(\"listings\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "  r.listing_id,\n",
    "  AVG(LENGTH(r.comments)) AS average_comment_length,\n",
    "  COUNT(r.id) AS reviews_count\n",
    "FROM\n",
    "  reviews r\n",
    "JOIN\n",
    "  listings l\n",
    "  ON r.listing_id = l.id\n",
    "GROUP BY\n",
    "  r.listing_id\n",
    "HAVING\n",
    "  COUNT(r.id) >= 5\n",
    "ORDER BY\n",
    "  average_comment_length DESC\n",
    "\"\"\"\n",
    "\n",
    "spark \\\n",
    "  .sql(sql_query) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd68c71-0ce8-4a21-be62-a822fce18522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.3/libexec/python/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------+\n",
      "|host_id|average_days_since_first_review_days|\n",
      "+-------+------------------------------------+\n",
      "|   4775|                  3046.6666666666665|\n",
      "|   6774|                  1763.1666666666667|\n",
      "|   9089|                               246.0|\n",
      "|   9323|                              2756.0|\n",
      "|  11431|                              3352.0|\n",
      "|  14596|                              2439.0|\n",
      "|  19195|                              3403.0|\n",
      "|  24334|                               133.0|\n",
      "|  25235|                              1686.0|\n",
      "|  26648|                              2387.0|\n",
      "|  30577|                               730.0|\n",
      "|  30780|                              3040.0|\n",
      "|  32851|                              3273.5|\n",
      "|  34007|                               108.0|\n",
      "|  36808|                               510.0|\n",
      "|  38691|                               737.0|\n",
      "|  40515|                              1999.0|\n",
      "|  40944|                             487.875|\n",
      "|  41759|                              5129.0|\n",
      "|  43039|                              4578.0|\n",
      "+-------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. [Optional][Challenge]\n",
    "# Calculate an average time passed from the first review for each listing in the listings dataset. \n",
    "# To implmenet a custom aggregation function you would need to use \"pandas_udf\" function to write a custom aggregation function.\n",
    "#\n",
    "# Documentation about \"pandas_udf\": https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html \n",
    "#\n",
    "# To use \"pandas_udf\" you would need to install two additional dependencies in the virtual environment you use for PySpark:\n",
    "# Run these commands:\n",
    "# ```\n",
    "# pip install pandas\n",
    "# pip install pyarrow\n",
    "# ```\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@pandas_udf(DoubleType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def average_days_since_first_review_udf(first_review_series) -> float:\n",
    "    today = pd.to_datetime('today')\n",
    "    listing_ages = (today - pd.to_datetime(first_review_series)).dt.days\n",
    "    if listing_ages.empty:\n",
    "        return None\n",
    "    return listing_ages.mean()\n",
    "\n",
    "listings \\\n",
    "  .filter(\n",
    "    listings.first_review.isNotNull()\n",
    "  ) \\\n",
    "  .groupBy('host_id') \\\n",
    "  .agg(\n",
    "    average_days_since_first_review_udf(listings.first_review).alias('average_days_since_first_review_days')\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b6881-82df-445d-80e0-56b390b305d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
